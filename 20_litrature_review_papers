1.
Mem0: Building production-ready ai agents with scalable long-term memory
2025
⋅
20 Citations
⋅
Google Scholar
Prateek Chhikara ... +4 more
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 66/100
Relevance Tag: Somewhat Relevant
Reasoning: Uses a tiered memory design to separate short‑term and long‑term stores, employs a vector DB (with limited relational integration), implements token‑budgeted retrieval and summarization, showcases demos in chat‑like tasks, and provides open‑source code plus benchmark results.
	
2.
ENGRAM: Effective, Lightweight Memory Orchestration for Conversational Agents
2025
⋅
arXiv
Daivik Patel ... +1 more
Open Access
- Preprint

Cite

PDF


Relevance Score: 61/100
Relevance Tag: Somewhat Relevant
Reasoning: Uses three typed memory tiers and a relational schema with embeddings; retrieves dense neighbors via simple set ops; lacks token‑budgeting, task‑oriented demos, and full reproducibility details.
Large language models (LLMs) deployed in user-facing applications require long-horizon consistency: the ability to remember prior interactions, respect user preferences, and ground reasoning in past events. However, contemporary memory systems often adopt complex architectures such as knowledge graphs, multi-stage retrieval pipelines, and OS-style schedulers, which introduce engineering complexity and reproducibility challenges. We present ENGRAM, a lightweight memory system that organizes conversation into three canonical memory types (episodic, semantic, and procedural) through a single router and retriever. Each user turn is converted into typed memory records with normalized schemas and embeddings and stored in a database. At query time, the system retrieves top-k dense neighbors for each type, merges results with simple set operations, and provides the most relevant evidence as context to the model. ENGRAM attains state-of-the-art results on LoCoMo, a multi-session conversational QA benchmark for long-horizon memory, and exceeds the full-context baseline by 15 points on LongMemEval while using only about 1% of the tokens. These results show that careful memory typing and straightforward dense retrieval can enable effective long-term memory management in language models without requiring complex architectures.

	
3.
Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in LLMs
2025
⋅
arXiv
Mohammad Tavakoli ... +5 more
Open Access
- Preprint

Cite

PDF


Relevance Score: 55/100
Relevance Tag: Somewhat Relevant
Reasoning: Uses three-tier memory (episodic, working, scratchpad); no hybrid DB layer; includes retrieval and summarization hints but lacks explicit token budgeting; evaluated on long conversational benchmark; provides quantitative experiments and likely open resources.
Evaluating the abilities of large language models (LLMs) for tasks that require long-term memory and thus long-context reasoning, for example in conversational settings, is hampered by the existing benchmarks, which often lack narrative coherence, cover narrow domains, and only test simple recall-oriented tasks. This paper introduces a comprehensive solution to these challenges. First, we present a novel framework for automatically generating long (up to 10M tokens), coherent, and topically diverse conversations, accompanied by probing questions targeting a wide range of memory abilities. From this, we construct BEAM, a new benchmark comprising 100 conversations and 2,000 validated questions. Second, to enhance model performance, we propose LIGHT-a framework inspired by human cognition that equips LLMs with three complementary memory systems: a long-term episodic memory, a short-term working memory, and a scratchpad for accumulating salient facts. Our experiments on BEAM reveal that even LLMs with 1M token context windows (with and without retrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT consistently improves performance across various models, achieving an average improvement of 3.5%-12.69% over the strongest baselines, depending on the backbone LLM. An ablation study further confirms the contribution of each memory component.

	
4.
MemGPT: Towards LLMs as Operating Systems
2023
⋅
56 Citations
⋅
SciSpace
Charles Packer ... +5 more
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 53/100
Relevance Tag: Somewhat Relevant
Reasoning: Uses hierarchical fast/slow memory tiers; lacks SQL‑vector hybrid. Implements token budgeting and summarization for context. Demonstrates long‑term chat agents. Provides open‑source code and quantitative benchmarks.
Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the

5.
A-mem: Agentic memory for llm agents
2025
⋅
144 Citations
⋅
Google Scholar
Wujiang Xu ... +5 more
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 50/100
Relevance Tag: Somewhat Relevant
Reasoning: Uses structured memory notes to capture info, includes token‑budgeting and summarization, provides quantitative experiments with open code, demos limited conversational tasks, but lacks explicit multi‑tier tiers and hybrid SQL‑vector storage.


	
6.
Multiple Memory Systems for Enhancing the Long-term Memory of Agent
2025
⋅
SciSpace
Gaoke Zhang ... +4 more
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 48/100
Relevance Tag: Marginally Relevant
Reasoning: Uses multi‑stage short‑term to long‑term fragments; lacks relational‑vector hybrid. Retrieves relevant units but no token‑budgeting. Tested on dialogue dataset; provides experiments and ablations.
An agent powered by large language models have achieved impressive results, but effectively handling the vast amounts of historical data generated during interactions remains a challenge. The current approach is to design a memory module for the agent to process these data. However, existing methods, such as MemoryBank and A-MEM, have poor quality of stored memory content, which affects recall performance and response quality. In order to better construct high-quality long-term memory content, we have designed a multiple memory system (MMS) inspired by cognitive psychology theory. The system processes short-term memory to multiple long-term memory fragments, and constructs retrieval memory units and contextual memory units based on these fragments, with a one-to-one correspondence between the two. During the retrieval phase, MMS will match the most relevant retrieval memory units based on the user's query. Then, the corresponding contextual memory units is obtained as the context for the response stage to enhance knowledge, thereby effectively utilizing historical data. Experiments on LoCoMo dataset compared our method with three others, proving its effectiveness. Ablation studies confirmed the rationality of our memory units. We also analyzed the robustness regarding the number of selected memory segments and the storage overhead, demonstrating its practical value.

	
7.
Toward Edge General Intelligence with Agentic AI and Agentification: Concepts, Technologies, and Future Directions
2025
⋅
SciSpace Full Text
Ruichen Zhang ... +12 more
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 48/100
Relevance Tag: Marginally Relevant
Reasoning: Uses multi‑tier memory (working, long‑term, episodic, semantic) and vector stores; mentions retrieval‑augmented generation and context windows but lacks relational DB, token‑budgeting details, conversational demos, and full reproducibility artifacts.
The rapid expansion of sixth-generation (6G) wireless networks and the Internet of Things (IoT) has catalyzed the evolution from centralized cloud intelligence towards decentralized edge general intelligence. However, traditional edge intelligence methods, characterized by static models and limited cognitive autonomy, fail to address the dynamic, heterogeneous, and resource-constrained scenarios inherent to emerging edge networks. Agentic artificial intelligence (Agentic AI) emerges as a transformative solution, enabling edge systems to autonomously perceive multimodal environments, reason contextually, and adapt proactively through continuous perception-reasoning-action loops. In this context, the agentification of edge intelligence serves as a key paradigm shift, where distributed entities evolve into autonomous agents capable of collaboration and continual adaptation. This paper presents a comprehensive survey dedicated to Agentic AI and agentification frameworks tailored explicitly for edge general intelligence. First, we systematically introduce foundational concepts and clarify distinctions from traditional edge intelligence paradigms. Second, we analyze important enabling technologies, including compact model compression, energy-aware computing strategies, robust connectivity frameworks, and advanced knowledge representation and reasoning mechanisms. Third, we provide representative case studies demonstrating Agentic AI's capabilities in low-altitude economy networks, intent-driven networking, vehicular networks, and human-centric service provisioning, supported by numerical evaluations. Furthermore, we identify current research challenges, review emerging open-source platforms, and highlight promising future research directions to guide robust, scalable, and trustworthy Agentic AI deployments for next-generation edge environments.

	
8.
MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications
2025
⋅
arXiv
Stefano Zeppieri
Open Access
- Preprint

Cite

PDF


Relevance Score: 46/100
Relevance Tag: Marginally Relevant
Reasoning: Uses five-layer memory model; no hybrid DB integration; mentions retrieval and latency but lacks token budgeting; demonstrated in Heero chat agent; lacks quantitative, reproducible experiments.
Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.

	
9.
MIRIX: Multi-Agent Memory System for LLM-Based Agents
2025
⋅
SciSpace
Yu Wang ... +1 more
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 46/100
Relevance Tag: Marginally Relevant
Reasoning: Uses six-tier memory (Core, Episodic, Semantic, Procedural, Resource, Knowledge Vault). No hybrid SQL‑vector store. No token‑budgeting mechanisms. Evaluated on long-form conversation benchmark (LOCOMO). Provides benchmarks and a packaged app, but reproducibility details are limited.
Although memory capabilities of AI agents are gaining increasing attention, existing solutions remain fundamentally limited. Most rely on flat, narrowly scoped memory components, constraining their ability to personalize, abstract, and reliably recall user-specific information over time. To this end, we introduce MIRIX, a modular, multi-agent memory system that redefines the future of AI memory by solving the field's most critical challenge: enabling language models to truly remember. Unlike prior approaches, MIRIX transcends text to embrace rich visual and multimodal experiences, making memory genuinely useful in real-world scenarios. MIRIX consists of six distinct, carefully structured memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and Knowledge Vault, coupled with a multi-agent framework that dynamically controls and coordinates updates and retrieval. This design enables agents to persist, reason over, and accurately retrieve diverse, long-term user data at scale. We validate MIRIX in two demanding settings. First, on ScreenshotVQA, a challenging multimodal benchmark comprising nearly 20,000 high-resolution computer screenshots per sequence, requiring deep contextual understanding and where no existing memory systems can be applied, MIRIX achieves 35% higher accuracy than the RAG baseline while reducing storage requirements by 99.9%. Second, on LOCOMO, a long-form conversation benchmark with single-modal textual input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing existing baselines. These results show that MIRIX sets a new performance standard for memory-augmented LLM agents. To allow users to experience our memory system, we provide a packaged application powered by MIRIX. It monitors the screen in real time, builds a personalized memory base, and offers intuitive visualization and secure local storage to ensure privacy.

	
10.
Memory-Augmented Large Language Model for Enhanced Chatbot Services in University Learning Management Systems
2025
⋅
SciSpace
J.-H. Lee ... +1 more
Applied Sciences
Journal Article

Cite

PDF

DOI


Relevance Score: 43/100
Relevance Tag: Marginally Relevant
Reasoning: Uses short‑term, long‑term, and temporal memories; lacks detailed tier taxonomy. No hybrid SQL‑vector store described. Implements retrieval filtering and ranking but no explicit token‑budgeting. Demonstrates LMS chatbot use‑case. Provides quantitative and user‑study evaluation, though reproducibility details are limited.
A learning management system (LMS) plays a crucial role in supporting students’ educational activities by centralized platforms for course delivery, communication, and student support. Recently, many universities have integrated chatbots into their LMS to assist students with various inquiries and tasks. However, existing chatbots often necessitate human interventions to manually respond to complex queries,


11.
Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects
2025
⋅
arXiv
Chris Latimer ... +6 more
Open Access
- Preprint

Cite

PDF


Relevance Score: 43/100
Relevance Tag: Marginally Relevant
Reasoning: Uses four logical networks to structure memory, but not the classic tier taxonomy; no SQL‑vector hybrid layer; offers retain/recall/reflect mechanisms that touch token budgeting; evaluated on long‑conversation benchmarks, showing strong gains; provides quantitative results with an open‑source model but limited details on full reproducibility.
Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.

	
12.
CogEvo-Edu: Cognitive Evolution Educational Multi-Agent Collaborative System
2025
⋅
arXiv
Yefeng Wu ... +4 more
Open Access
- Preprint

Cite

PDF


Relevance Score: 43/100
Relevance Tag: Marginally Relevant
Reasoning: Uses dual memories and consolidation to manage student profiles; lacks explicit relational‑vector store; employs adaptive retrieval and forgetting but no token‑budgeting; demonstrated in DSP tutoring dialogue; provides benchmark results but limited reproducibility details.
Large language models (LLMs) are increasingly deployed as conversational tutors in STEM education, yet most systems still rely on a single LLM with a static retrieval-augmented generation (RAG) pipeline over course materials. This design struggles in complex domains such as digital signal processing (DSP), where tutors must maintain coherent long-term student models, manage heterogeneous knowledge bases, and adapt teaching strategies over extended interactions. We argue that retrieval, memory, and control should be treated as a coupled cognitive evolution process. We instantiate this view in CogEvo-Edu, a hierarchical educational multi-agent system comprising a Cognitive Perception Layer (CPL), a Knowledge Evolution Layer (KEL), and a Meta-Control Layer (MCL). CPL maintains dual memories and performs confidence-weighted consolidation to build structured, self-correcting student profiles under limited context. KEL assigns each knowledge chunk a spatiotemporal value that drives activation, semantic compression, and forgetting. MCL formulates tutoring as hierarchical sequential decision making, orchestrating specialized agents and jointly adapting CPL/KEL hyperparameters via a dual inner--outer loop. To evaluate CogEvo-Edu, we construct DSP-EduBench, a vertical benchmark for DSP tutoring with heterogeneous resources, simulated student profiles, and long-horizon interaction scripts. Using a three-model LLM-as-a-Judge ensemble, CogEvo-Edu raises the overall score from 5.32 to 9.23 and improves all six indicators over static RAG, simple memory, and a single-agent variant, demonstrating the value of jointly evolving student profiles, knowledge bases, and teaching policies.

	
13.
Practical Considerations for Agentic LLM Systems
2024
⋅
SciSpace Full Text
Chris Sypherd ... +1 more
Journal Article

Cite

PDF

DOI


Relevance Score: 40/100
Relevance Tag: Marginally Relevant
Reasoning: Discusses long‑term memory storage, RAG with vector DBs, and selective retrieval, but lacks explicit multi‑tier design, relational‑vector hybrid, token‑budget strategies, concrete conversational demos, and reproducible experiments.
As the strength of Large Language Models (LLMs) has grown over recent years, so too has interest in their use as the underlying models for autonomous agents. Although LLMs demonstrate emergent abilities and broad expertise across natural language domains, their inherent unpredictability makes the implementation of LLM agents challenging, resulting in a gap between related research and the real-world implementation of such systems. To bridge this gap, this paper frames actionable insights and considerations from the research community in the context of established application paradigms to enable the construction and facilitate the informed deployment of robust LLM agents. Namely, we position relevant research findings into four broad categories--Planning, Memory, Tools, and Control Flow--based on common practices in application-focused literature and highlight practical considerations to make when designing agentic LLMs for real-world applications, such as handling stochasticity and managing resources efficiently. While we do not conduct empirical evaluations, we do provide the necessary background for discussing critical aspects of agentic LLM designs, both in academia and industry.

	
14.
Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review
2025
⋅
SciSpace
Aditya Nagori ... +4 more
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 40/100
Relevance Tag: Marginally Relevant
Reasoning: Uses Neo4j KG plus FAISS vectors for hybrid storage; evaluates agent on synthetic benchmarks with open data and code. Lacks explicit multi‑tier memory or token‑budgeting, and focuses on literature review rather than dialogue agents.
The surge in scientific publications challenges traditional review methods, demanding tools that integrate structured metadata with full-text analysis. Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries with vector search offer promise but are typically static, rely on proprietary tools, and lack uncertainty estimates. We present an agentic approach that encapsulates the

15.
Survey on Evaluation of LLM-based Agents
2025
⋅
SciSpace Full Text
Asaf Yehudai ... +7 more
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 40/100
Relevance Tag: Marginally Relevant
Reasoning: Uses tiered memory examples (MemGPT, A-MEM) and benchmarks like LTMbenchmark, but lacks detailed multi‑tier design, hybrid storage, explicit token‑budgeting, and clear reproducible code links.
	
16.
RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong Learning in Physical Embodied Systems
2025
⋅
SciSpace
Mingcong Lei ... +14 more
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 38/100
Relevance Tag: Marginally Relevant
Reasoning: Uses multi‑memory tiers (spatial, temporal, episodic, semantic) for embodied agents; lacks hybrid SQL‑vector storage; offers limited context budgeting; focuses on robot tasks, not dialogue; provides benchmarks and ablations but reproducibility details are unclear.
We present RoboMemory, a brain-inspired multi-memory framework for lifelong learning in physical embodied systems, addressing critical challenges in real-world environments: continuous learning, multi-module memory latency, task correlation capture, and infinite-loop mitigation in closed-loop planning. Grounded in cognitive neuroscience, it integrates four core modules: the Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System (hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and the Low-Level Executer (cerebellum-like) to enable long-term planning and cumulative learning. The Lifelong Embodied Memory System, central to the framework, alleviates inference speed issues in complex memory frameworks via parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent architectural design to enhance memory consistency and scalability. Evaluations on EmbodiedBench show RoboMemory outperforms the open-source baseline (Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing new SOTA. Ablation studies validate key components (critic, spatial memory, long-term memory), while real-world deployment confirms its lifelong learning capability with significantly improved success rates across repeated tasks. RoboMemory alleviates high latency challenges with scalability, serving as a foundational reference for integrating multi-modal memory systems in physical robots.

	
17.
Fixed-Persona SLMs with Modular Memory: Scalable NPC Dialogue on Consumer Hardware
2025
⋅
arXiv
Martin Braas ... +1 more
Open Access
- Preprint

Cite

PDF


Relevance Score: 38/100
Relevance Tag: Marginally Relevant
Reasoning: Uses modular, swappable memory modules for NPC personas, enabling expressive dialogue; lacks explicit multi‑tier or hybrid DB design and token‑budgeting; evaluated on consumer hardware with open‑source SLMs, providing reproducible benchmarks for conversational agents.
Large Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet their applicability to dialogue systems in computer games remains limited. This limitation arises from their substantial hardware requirements, latency constraints, and the necessity to maintain clearly defined knowledge boundaries within a game setting. In this paper, we propose a modular NPC dialogue system that leverages Small Language Models (SLMs), fine-tuned to encode specific NPC personas and integrated with runtime-swappable memory modules. These memory modules preserve character-specific conversational context and world knowledge, enabling expressive interactions and long-term memory without retraining or model reloading during gameplay. We comprehensively evaluate our system using three open-source SLMs: DistilGPT-2, TinyLlama-1.1B-Chat, and Mistral-7B-Instruct, trained on synthetic persona-aligned data and benchmarked on consumer-grade hardware. While our approach is motivated by applications in gaming, its modular design and persona-driven memory architecture hold significant potential for broader adoption in domains requiring expressive, scalable, and memory-rich conversational agents, such as virtual assistants, customer support bots, or interactive educational systems.

	
18.
TAMEing Long Contexts in Personalization: Towards Training-Free and State-Aware MLLM Personalized Assistant
2025
⋅
arXiv
Rongpei Hong ... +4 more
Open Access
- Preprint

Cite

PDF

DOI


Relevance Score: 38/100
Relevance Tag: Marginally Relevant
Reasoning: Uses double memories to track temporal and persistent concept variations; focuses on personalized dialogue assistants; provides benchmark results and likely open code, but lacks multi‑tier, hybrid DB, and token‑budgeting mechanisms.
Multimodal Large Language Model (MLLM) Personalization is a critical research problem that facilitates personalized dialogues with MLLMs targeting specific entities (known as personalized concepts). However, existing methods and benchmarks focus on the simple, context-agnostic visual identification and textual replacement of the personalized concept (e.g., -> ), overlooking the ability to support long-context conversations. An ideal personalized MLLM assistant is capable of engaging in long-context dialogues with humans and continually improving its experience quality by learning from past dialogue histories. To bridge this gap, we propose LCMP, the first Long-Context MLLM Personalization evaluation benchmark. LCMP assesses the capability of MLLMs in perceiving variations of personalized concepts and generating contextually appropriate personalized responses that reflect these variations. As a strong baseline for LCMP, we introduce a novel training-free and state-aware framework TAME. TAME endows MLLMs with double memories to manage the temporal and persistent variations of each personalized concept in a differentiated manner. In addition, TAME incorporates a new training-free Retrieve-then-Align Augmented Generation (RA2G) paradigm. RA2G introduces an alignment step to extract the contextually fitted information from the multi-memory retrieved knowledge to the current questions, enabling better interactions for complex real-world user queries. Experiments on LCMP demonstrate that TAME achieves the best performance, showcasing remarkable and evolving interaction experiences in long-context scenarios.

19.
Cognitive Workspace: Active Memory Management for LLMs - An Empirical Study of Functional Infinite Context
2025
⋅
SciSpace
Tao An
arXiv.org
Journal Article

Cite

PDF

DOI


Relevance Score: 37/100
Relevance Tag: Marginally Relevant
Reasoning: Uses hierarchical cognitive buffers for active memory, implements task‑driven context optimization with token budgeting, but lacks explicit multi‑tier taxonomy, relational‑vector storage, and concrete conversational demos; provides quantitative results though reproducibility details are limited.
Large Language Models (LLMs) face fundamental limitations in context management despite recent advances extending context windows to millions of tokens. We propose Cognitive Workspace, a novel paradigm that transcends traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive mechanisms of external memory use. Drawing from cognitive science foundations including Baddeley's working memory model, Clark's extended mind thesis, and Hutchins'distributed cognition framework, we demonstrate that current passive retrieval systems fail to capture the dynamic, task-driven nature of human memory management. Our analysis of 2024-2025 developments reveals that while techniques like Infini-attention and StreamingLLM achieve impressive context lengths, they lack the metacognitive awareness and active planning capabilities essential for true cognitive extension. Cognitive Workspace addresses these limitations through three core innovations: (1) active memory management with deliberate information curation, (2) hierarchical cognitive buffers enabling persistent working states, and (3) task-driven context optimization that dynamically adapts to cognitive demands. Empirical validation demonstrates Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from 54-60% across different tasks) compared to 0% for traditional RAG, with 17-18% net efficiency gain despite 3.3x higher operation counts. Statistical analysis confirms these advantages with p<0.001 and Cohen's d>23 across multiple task types, establishing the first quantitative evidence for active memory superiority in LLM systems. We present a comprehensive theoretical framework synthesizing insights from 50+ recent papers, positioning Cognitive Workspace as a fundamental shift from information retrieval to genuine cognitive augmentation.

	
20.
&lt;p&gt;Memory Architecture in S-AI-GPT: From Contextual Adaptation to Hormonal Modulation&lt;/p&gt;
2025
⋅
SciSpace
Said Slaoui
Social Science Research Network
Repository

Cite

PDF

DOI


Relevance Score: 36/100
Relevance Tag: Marginally Relevant
Reasoning: Uses a three‑tier memory (working, long‑term, affective) in a conversational GPT framework; lacks relational‑vector hybrid storage; offers adaptive forgetting but no token‑budgeting; demonstrates conversational use without quantitative benchmarks; no reproducible experiments provided.
A BSTRACT This article presents a biologically inspired memory architecture embedded within the Sparse Artificial Intelligence – Generative Pretrained Transformer (S-AI-GPT) conversational framework. Addressing the limitations of stateless Large Language Models (LLMs), the system integrates three complementary components: a Dynamic Contextual Memory (DCM) for short-term working retention, a GPTMemoryAgent for long-term personalized storage, and a GPT-MemoryGland for affective trace encoding and modulation. These components are orchestrated by a hormonal engine, enabling adaptive forgetting, emotional persistence, and context-aware prioritization of memory traces. Unlike typical passive memory modules, this architecture introduces an active, symbolic, and controllable memory system: memory traces can trigger internal hormonal signals, are stored in a structured and interpretable form, and can be selectively reinforced, inhibited, or reorganized by the GPT-MetaAgent. The proposed model provides a promising foundation for building frugal, adaptive, and explainable lifelong memory systems in conversational AI.

	
21.
A Simple Yet Strong Baseline for Long-Term Conversational Memory of LLM Agents
2025
⋅
arXiv
Sizhe Zhou ... +1 more
Open Access
- Preprint

Cite

PDF


Relevance Score: 36/100
Relevance Tag: Marginally Relevant
Reasoning: Uses event‑centric graph memory for long‑term dialogue; retrieves via dense search and LLM filtering. Evaluated on conversational benchmarks with open code, but lacks explicit tiered design, relational‑vector hybrid, and token‑budgeting mechanisms.
LLM-based conversational agents still struggle to maintain coherent, personalized interaction over many sessions: fixed context windows limit how much history can be kept in view, and most external memory approaches trade off between coarse retrieval over large chunks and fine-grained but fragmented views of the dialogue. Motivated by neo-Davidsonian event semantics, we propose an event-centric alternative that represents conversational history as short, event-like propositions which bundle together participants, temporal cues, and minimal local context, rather than as independent relation triples or opaque summaries. In contrast to work that aggressively compresses or forgets past content, our design aims to preserve information in a non-compressive form and make it more accessible, rather than more lossy. Concretely, we instruct an LLM to decompose each session into enriched elementary discourse units (EDUs) -- self-contained statements with normalized entities and source turn attributions -- and organize sessions, EDUs, and their arguments in a heterogeneous graph that supports associative recall. On top of this representation we build two simple retrieval-based variants that use dense similarity search and LLM filtering, with an optional graph-based propagation step to connect and aggregate evidence across related EDUs. Experiments on the LoCoMo and LongMemEval$_S$ benchmarks show that these event-centric memories match or surpass strong baselines, while operating with much shorter QA contexts. Our results suggest that structurally simple, event-level memory provides a principled and practical foundation for long-horizon conversational agents. Our code and data will be released at https://github.com/KevinSRR/EMem.

	
22.
PersonaMem-v2: Towards Personalized Intelligence via Learning Implicit User Personas and Agentic Memory
2025
⋅
arXiv
Bowen Jiang ... +15 more
Open Access
- Preprint

Cite

PDF


Relevance Score: 36/100
Relevance Tag: Marginally Relevant
Reasoning: Uses a single, human‑readable memory instead of multi‑tier; no relational‑vector hybrid. Implements token budgeting by replacing full context with 2k‑token memory. Demonstrates personalization in realistic chatbot dialogues. Provides quantitative benchmarks and dataset, supporting reproducibility.
Personalization is one of the next milestones in advancing AI capability and alignment. We introduce PersonaMem-v2, the state-of-the-art dataset for LLM personalization that simulates 1,000 realistic user-chatbot interactions on 300+ scenarios, 20,000+ user preferences, and 128k-token context windows, where most user preferences are implicitly revealed to reflect real-world interactions. Using this data, we investigate how reinforcement fine-tuning enables a model to improve its long-context reasoning capabilities for user understanding and personalization. We also develop a framework for training an agentic memory system, which maintains a single, human-readable memory that grows with each user over time. In our experiments, frontier LLMs still struggle with implicit personalization, achieving only 37-48% accuracy. While they support long context windows, reasoning remains the bottleneck for implicit personalization tasks. Using reinforcement fine-tuning, we successfully train Qwen3-4B to outperforms GPT-5, reaching 53% accuracy in implicit personalization. Moreover, our agentic memory framework achieves state-of-the-art 55% accuracy while using 16x fewer input tokens, relying on a 2k-token memory instead of full 32k conversation histories. These results underscore the impact of our dataset and demonstrate agentic memory as a scalable path toward real-world personalized intelligence.

23.
Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory
2025
⋅
arXiv
Tianxin Wei ... +14 more
Open Access
- Preprint

Cite

PDF


Relevance Score: 36/100
Relevance Tag: Marginally Relevant
Reasoning: Uses retrieval and update pipelines for dynamic context; evaluates on multi‑turn goal‑oriented tasks; provides open benchmark, code, and quantitative results.
Statefulness is essential for large language model (LLM) agents to perform long-term planning and problem-solving. This makes memory a critical component, yet its management and evolution remain largely underexplored. Existing evaluations mostly focus on static conversational settings, where memory is passively retrieved from dialogue to answer queries, overlooking the dynamic ability to accumulate and reuse experience across evolving task streams. In real-world environments such as interactive problem assistants or embodied agents, LLMs are required to handle continuous task streams, yet often fail to learn from accumulated interactions, losing valuable contextual insights, a limitation that calls for test-time evolution, where LLMs retrieve, integrate, and update memory continuously during deployment. To bridge this gap, we introduce Evo-Memory, a comprehensive streaming benchmark and framework for evaluating self-evolving memory in LLM agents. Evo-Memory structures datasets into sequential task streams, requiring LLMs to search, adapt, and evolve memory after each interaction. We unify and implement over ten representative memory modules and evaluate them across 10 diverse multi-turn goal-oriented and single-turn reasoning and QA datasets. To better benchmark experience reuse, we provide a baseline method, ExpRAG, for retrieving and utilizing prior experience, and further propose ReMem, an action-think-memory refine pipeline that tightly integrates reasoning, task actions, and memory updates to achieve continual improvement.

	
24.
LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning
2025
⋅
arXiv
Zhengjun Huang ... +6 more
Open Access
- Preprint

Cite

PDF


Relevance Score: 36/100
Relevance Tag: Marginally Relevant
Reasoning: Uses hierarchical graph for retrieval; lacks explicit multi‑tier or SQL‑vector hybrid. Provides retrieval ranking but no token‑budgeting. Evaluated on dialogue benchmarks; open‑source code and metrics enable reproducibility.
Large Language Model (LLM) agents exhibit remarkable conversational and reasoning capabilities but remain constrained by limited context windows and the lack of persistent memory. Recent efforts address these limitations via external memory architectures, often employing graph-based representations, yet most adopt flat, entangled structures that intertwine semantics with topology, leading to redundant representations, unstructured retrieval, and degraded efficiency and accuracy. To resolve these issues, we propose LiCoMemory, an end-to-end agentic memory framework for real-time updating and retrieval, which introduces CogniGraph, a lightweight hierarchical graph that utilizes entities and relations as semantic indexing layers, and employs temporal and hierarchy-aware search with integrated reranking for adaptive and coherent knowledge retrieval. Experiments on long-term dialogue benchmarks, LoCoMo and LongMemEval, show that LiCoMemory not only outperforms established baselines in temporal reasoning, multi-session consistency, and retrieval efficiency, but also notably reduces update latency. Our official code and data are available at https://github.com/EverM0re/LiCoMemory.

	
25.
Memtrack: Evaluating long-term memory and state tracking in multi-platform dynamic agent environments
1 Citations
⋅
Google Scholar
D Deshpande ... +2 more

Cite

PDF


Relevance Score: 35/100
Relevance Tag: Marginally Relevant
Reasoning: Uses a layered memory module (MEM0) to store episodic info, but lacks explicit tier definitions; no hybrid SQL‑vector store; no token‑budgeting mechanisms; tests agents in multi‑platform tasks resembling dialogue; provides quantitative benchmarks and open‑source code for reproducibility.
	
26.
Relational Memory Augmented Language Models
2022
⋅
SciSpace
Open Access
- Posted Content

Cite

PDF

DOI


Relevance Score: 35/100
Relevance Tag: Marginally Relevant
Reasoning: Uses knowledge‑graph triples to augment LM, showing quantitative gains on standard corpora; integrates relational memory with token‑based memory but lacks multi‑tier agent design, hybrid DB implementation, budgeting strategies, or conversational agent demos.
We present a memory-augmented approach to condition an autoregressive language model on a knowledge graph. We represent the graph as a collection of relation triples and retrieve relevant relations for a given context to improve text generation. Experiments on WikiText-103, WMT19, and enwik8 English datasets demonstrate that our approach produces a better language model in terms
